# Lander Compete

<h2>Introduction :</h2>
The main purpuse of this project to do some experiment on the three well knowns Reinforcement Algorithms DQN, DDPG and SARSA in terms of their learning pace,
reward, average reward over a certain number of episodes, senstivity to the change in state and ...... # To think
<h3>Models Used :</h3>
<ul>
 
 <li><B>DQN -> </B>Deep Q Networks (DQN) are neural networks (and/or related tools) that utilize deep Q learning in order to provide models such as the simulation of intelligent video game play. Rather than being a specific name for a specific neural network build, Deep Q Networks may be composed of convolutional neural networks and other structures that use specific methods to learn about various processes.</li>
 <li><B>DDPG -> </B>
 Deep Deterministic Policy Gradient (DDPG) is a model-free off-policy algorithm for learning continous actions.

It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.</li>
 
 <li><B>SARSA -> </B>State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name "Modified Connectionist Q-Learning" (MCQ-L).</li>
 
</ul>










 
 
 Compare the outputs from these models and see the results. <br>
 Use Tensorboard (Or something else) to see the live changes in accuracy Or losses. <br>
 Last but not the list. Do Something Visionary. <br>
