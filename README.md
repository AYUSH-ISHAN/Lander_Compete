# RL_Models_Comparision

<h2>Introduction :</h2>
<h3>Models Used :</h3>
<ul>
 
 <li><B>DQN -> </B>Deep Q Networks (DQN) are neural networks (and/or related tools) that utilize deep Q learning in order to provide models such as the simulation of intelligent video game play. Rather than being a specific name for a specific neural network build, Deep Q Networks may be composed of convolutional neural networks and other structures that use specific methods to learn about various processes.</li>
 <li><B>DDPG -> </B>
 Deep Deterministic Policy Gradient (DDPG) is a model-free off-policy algorithm for learning continous actions.

It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.</li>
 
 <li><B>SARSA -> </B>State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name "Modified Connectionist Q-Learning" (MCQ-L).</li>
 
</ul>










# Soon I will be drafting the Results, Analysis and Comparisions of the models. 
 
 Compare the outputs from these models and see the results. <br>
 Use Tensorboard (Or something else) to see the live changes in accuracy Or losses. <br>
 Last but not the list. Do Something Visionary. <br>
